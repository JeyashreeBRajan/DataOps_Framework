name: Run Weather ETL DAG

on:
  workflow_dispatch:  # Allows manual trigger

jobs:
  run-dag:
    runs-on: ubuntu-latest

    env:
      AIRFLOW_HOME: ${{ github.workspace }}/airflow_home
      PYTHONPATH: ${{ github.workspace }}

    steps:
      # 1️⃣ Checkout the repo
      - name: Checkout code
        uses: actions/checkout@v3

      # 2️⃣ Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.11

      # 3️⃣ Upgrade pip and install Cython first
      - name: Upgrade pip and install build tools
        run: |
          python -m pip install --upgrade pip setuptools wheel Cython

      # 4️⃣ Install Airflow + dependencies
      - name: Install dependencies
        run: |
          pip install "apache-airflow==2.9.3" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.9.3/constraints-3.11.txt"
          pip install pandas numpy psycopg2-binary soda-sql

      # 5️⃣ Create Airflow directories and copy DAGs
      - name: Create Airflow directories
        run: |
          mkdir -p $AIRFLOW_HOME/dags
          mkdir -p $AIRFLOW_HOME/logs
          mkdir -p $AIRFLOW_HOME/plugins
          cp -r e_dags/* $AIRFLOW_HOME/dags/

      # 6️⃣ Initialize Airflow database
      - name: Initialize Airflow database
        run: airflow db init

      # 7️⃣ List all DAGs to ensure registration
      - name: List all DAGs
        run: airflow dags list

      # 8️⃣ Trigger the Weather ETL DAG
      - name: Trigger Weather ETL DAG
        run: airflow dags trigger weather_pipeline
